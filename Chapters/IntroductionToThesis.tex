%!TEX root = ..\main.tex
\chapter{Introduction to this Thesis}
\label{Ch:ThesisIntro}

\lhead{Chapter \ref{Ch:ThesisIntro}. \emph{Introduction to this Thesis}} % This is for the header on each page

Initially, this thesis was intended to be made up entirely of the contents of Part \ref{part:optimization for stats}, along with what we hoped would be several significant further contributions to the study. However, the practicalities of a deadline, along with the challenging nature of the research, meant that the decision was made to augment this thesis with an essentially separate section of study. This is what makes up Part \ref{part:coupling time}.

% This thesis is made up of two parts. Part \ref{part:coupling time} titled, \nameref{part:coupling time} and part \ref{part:optimization for stats} titled, \nameref{part:optimization for stats}. 
The reader should view these two parts as standalone topics, to be read independently. However, they are not without any commonality. Both are within the realm of stochastic mathematics, Part \ref{part:coupling time} being a study of a random variable constructed from a stochastic process, and Part \ref{part:optimization for stats} being a study of probability distributions that maximize certain statistical objective functions. 

% We note that while a typical thesis is a cohesive whole, this is certainly not a requirement to obtaining a doctorate. Indeed, to obtain the degree of Doctor of Philosphy, the candidate is required to produce a substantial piece of original research. We believe that the sum of the research contained in both parts of this thesis is sufficiently substantial, and thus adequate for submission.

Part \ref{part:coupling time} is titled ``The Coupling Time for the Ising Heat-Bath Dynamics.'' In it, we consider the Ising heat-bath Glauber dynamics on both the 1-dimensional cycle in Chapter \ref{Ch:1D} and on certain transitive graphs in Chapter \ref{Ch:GeneralResults}. These dynamics describe a continuous time Markov chain, whose states are assignments of spins (either $+1$ or $-1$) to each vertex in a given graph. We construct a coupling of two such Markov chains, one starting with all spins $+1$ and one starting with all spins $-1$. The time it takes for these coupled chains to have the same spin configuration is the coupling time.
Our main results show that the coupling time for these dynamics, when appropriately scaled, converges in distribution to a Gumbel distribution as the size of our graph goes to infinity. On the cycle, our results hold at all temperatures, and on certain transitive graphs, our results hold for sufficiently high temperatures. The two main tools we use are compound Poisson approximation, and the relatively new framework of information percolation, introduced by Lubetzky and Sly in \cite{Lubetzky2016-wd}. We use Chapter \ref{Ch:CouplingIntro} to summarise these techniques, as well as to introduce various preliminaries.

Part \ref{part:optimization for stats} is titled ``Efficient Optimization for Statistical Inference.'' In this part, we look at two optimization problems that arise in statistical inference. In Chapter \ref{Ch:Mixtures} we consider maximum likelihood mixtures where the components are parametrised by a single location parameter, and in Chapter \ref{Ch:Deconvolution} we look at a new method for deconvolution introduced by Delaigle and Hall in \cite{Delaigle2016-la}. In each of these, we have to solve some optimization problem to find a discrete probability distribution. A key similarity between these two problems is that the optimal probability distribution is typically supported on a very small number of points. 
For the case of maximum likelihood mixtures, there are some results in the literature bounding the number of points in these optimal distributions; however, we consider cases in which these bounds can be considerable overestimates. We provide various new results which provide tighter bounds in some cases or extend known bounds to different classes of component densities. We compare these to empirical solutions.
In the deconvolution case, the optimization problem is more complex than in the maximum likelihood setting, and obtaining and proving results concerning the number of points of support in the optimal distribution is difficult. However, we are still able to numerically explore the behaviour of these optimizing distributions and consider how we might take advantage of their properties.
