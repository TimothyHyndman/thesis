%!TEX root = ..\main.tex
\chapter{A Particular Class of Optimization Problem}
\label{Ch:StatOptim}

\lhead{Chapter \ref{Ch:StatOptim}. \emph{A Particular Class of Optimization 
Problem}} % This is for the header on each page

\section{Introduction}

"The results follow from this general theorem which seems obvious."

\begin{theorem}
	Let $(E_m)_{m=1}^\infty$ be a sequence of appropriately defined sets and let
	$(g_m)_{m=1}^\infty, g_m: E_m \mapsto \mathbb{R}$ be a sequence of
	functions that satisfy the following properties
	\begin{enumerate}
		\item $\forall \vect{x} \in \partial E_m, \exists n < m, \vect{y} \in E_n$ such that
		$g_m(\vect{x}) \leq g_n(\vect{y})$.
		\label{prop:one}
		\item $\exists m_0, \vect{x}_0 \in E_{m_0}$ such that $\forall m, \vect{x} \in 
		E_m$, $g_m(\vect{x}) \leq g_{m_0}(\vect{x}_0)$.
	\end{enumerate}
	Then $\exists m_*, \vect{x}_* \in E_{m_*} \setminus \partial E_{m_*}$ such
	that $\forall m, \vect{x} \in E_m$, $g_m(\vect{x}) \leq g_{m_*}(\vect{x}_*)$.
\end{theorem}
\proof{
	The proof is simple. If $\vect{x}_0 \notin \partial E_{m_0}$ then we are done.
	Otherwise, by property \ref{prop:one} we can find a $n$ and $\vect{y} \in 
	E_n$ such that $g_n(y) = g_{m_0}(\vect{x}_0)$. If $\vect{y} \notin \partial 
	E_n$ then we are done, otherwise we repeat the process until we find a $m, 
	\vect{x}$ pair with $\vect{x} \notin \partial E_m$. %Note that property 
	% \ref{prop:one} implies that $\partial E_1 = \emptyset$ and so this process
	% must end.
}



% Given a sequence of functions $(g_m)_{m=1}^\infty $, $g_m: \mathbb{R}^{m} \times 
% \mathbb{R}^m \mapsto \mathbb{R}$, find the $m$, $\vect{p} = (p_1, \dots, p_m)$ 
% and $\vect{\theta} = (\theta_1, \dots, \theta_m)$ that maximize
% \begin{equation}
% 	g_m\left( \vect{p}, \vect{\theta}	\right)
% 	\label{eq:objective}
% \end{equation}
% subject to
% \begin{align}
% 	p_j &\geq 0, \label{eq:constraint1}\\
% 	\sum_{j=1}^m p_j &= 1,\label{eq:constraint2}\\
% 	\theta_j &\leq \theta_{j+1} \label{eq:constraint3}
% \end{align}
% where the $g_m$ satisfies the following properties.

% \begin{align}
% 	g_m(p_1, \dots, p_{i-1}, 0, p_{i+1}, \dots, p_m, \vect{\theta}) 
% 		&= g_{m-1}(p_1, \dots, p_{i-1}, p_{i+1}, \dots, p_m, \theta_1, \dots,
% 		\theta_{i-1}, \theta_{i+1}, \dots, \theta_m)\\
% 		&= 
% \end{align}

% It will often prove more convenient for us to consider the equivalent problem 
% obtained by using \eqref{eq:constraint2} to eliminate $p_m$:

% Given functions $\vect{g}: \mathbb{R} \mapsto \mathbb{R}^n$, $h: \mathbb{R}^n 
% \mapsto \mathbb{R}$ and $m \in \mathbb{N^+}$, find the $\vect{p} = 
% (p_1, \dots, p_{m-1})$ and $\vect{\theta} = (\theta_1, \dots, \theta_m)$ that 
% maximize
% \begin{equation}
% 	h\left( \sum_{j=1}^{m-1} p_j \vect{g}(\theta_j) +
% 	\left(1 - \sum_{j=1}^{m-1} p_j \right) \vect{g}(\theta_m)
% 	\right)
% 	\label{eq:objective_alt}
% \end{equation}
% subject to
% \begin{align}
% 	p_j &\geq 0, \label{eq:constraint1_alt}\\
% 	\sum_{j=1}^{m-1} p_j &\leq 1,\label{eq:constraint2_alt}\\
% 	\theta_j &\leq \theta_{j+1}. \label{eq:constraint3_alt}
% \end{align}


% We make the following observation.
% \begin{theorem}
% 	If \eqref{eq:objective_alt} has a boundary solution ($p_i = 0$ or 
% 	$\theta_i = \theta_{i+1}$) for $m = M$, then there exists an $M^* < M$ such 
% 	that \eqref{eq:objective_alt} has an interior solution with the same value 
% 	for $m = M^*$.
% 	\label{thm:exists_interior_solution}
% \end{theorem}
% \begin{remark}
% 	If $g$ and $h$ are differentiable, then at an interior solution, all
% 	derivatives of our objective function are equal to zero.
% \end{remark}

% \begin{theorem}
% 	Max is always $m \leq n$??
% \end{theorem}

% \subsection{Examples}
% One problem that takes this form is that of finding a maximum likelihood
% mixture. Given data $\vect{x} = (x_1, \dots, x_n)$, and a component density
% $f(x; \theta)$, set 
% \begin{equation}
% 	\vect{g}(\theta) = (f(x_1; \theta), \dots, f(x_n; \theta))
% \end{equation}
% and
% \begin{equation}
% 	h(g_1, \dots, g_n) = \sum_{i = 1}^n \log(g_i).
% \end{equation}

\section{Maximum Likelihood Location Mixtures of Normals}
