%!TEX root = ..\..\main.tex
\chapter{Insights from Small \texorpdfstring{$n$}{n} Data}
\label{Ch:SmallNData}

\lhead{Chapter \ref{Ch:SmallNData}. \emph{Insights from Small \texorpdfstring{$n$}{n} Data}} % This is for the header on each page

%-------------------------------------------------------------------------------
%	Chapter Text
%-------------------------------------------------------------------------------

\section{Introduction}
	When we estimate the density of a random sample, $\vect{X} = X_1,X_2,\dots,X_n$, using a mixture model
	\begin{equation}
	f_Q(x) = \int_{-\infty}^{\infty} f_\theta(x) \intd Q(\theta)
	\end{equation}
	we often find that the mixing distribution, $Q$, that maximizes the log likelihood 
	\begin{equation}
	L(Q) = \sum_{i = 1}^n \log(f_Q(X_i))
	\label{smallndata:eq:likelihood}
	\end{equation}
	has only a few points of support. We can find insights into why this happens by looking at samples with small $n$. In this chapter we look in particular at the cases $n=2$ and $n=3$. Even though samples of this size are much smaller than those we would typically want to estimate the density of, the problem becomes significantly more simple (especially in the $n=2$ case) and some of the insights we gain can be applied to larger samples.

\section{Samples of size two}
	In section \ref{sec:mixturelikelihoods:geometrical}, we introduced the component likelihood vector 
	$$\vect{f}(\theta) = (f_\theta(X_1), \dots, f_\theta(X_n))$$
	and the set
	$$\Gamma = \{\vect{f}(\theta) : \theta \in \mathbb{R} \}.$$
	Now the analysis of $\Gamma$ is somewhat simplified when $\vect{f}(\theta)$ is two-dimensional (that is, $n = 2$)

\section{Phase Plots}
	We wish to make clear that the number of components in the MLE is purely a function of the data, $\vect{X} = (X_1, \dots, X_n)$. For any particular choice of $f_\theta$, we can think of the $n$ dimensional space of possible data sets as being partitioned by $C_k$, $k = 1,\dots,n$, where $C_k$ is the set of all $\vect{X}$ such that the MLE requires only $k$ components. The problem of determining the number of components required in the MLE for $\vect{X}$ becomes the problem of determining in which of the $C_k$ our data set lies for our choice of $f_\theta$.

	Let us consider the case where $f_\theta$ is normal with mean $\theta$ and variance $1$ and where $n=2$. From Theorem \ref{thm:distancebetweentwopoints},
	$$C_1 = \left \lbrace (X_1,X_2)\in \mathbb{R}^2 : |X_2 - X_1| \leq 2 \right\rbrace$$
	and so
	$$C_2 = \mathbb{R}^2 \setminus C_1.$$ We have plotted these two sets in Figure \ref{fig:phaseplotn2} for $-3<X_1,X_2<3$.
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{SmallNData/Sigma1n2res256}
		\caption{title}
		\label{fig:phaseplotn2}
	\end{figure}

	We can produce similar plots for the three dimensional case ($n=3$) by fixing $x_1$ and letting $x_2$ and $x_3$ vary. In Figure \ref{fig:phaseplotn3}, we have used the same $f_\theta$, and set $x_1 = 0$. We should note that choosing a different value for $x_1$ would still result in the same plot, just transposed along the line $x_3 = x_2$. Additionally, choosing a different variance for $f_\theta$ would result in a plot with the same relative shape, but different scale.

	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{SmallNData/Sigma1n3res1024width7}
		\caption{title}
		\label{fig:phaseplotn3}
	\end{figure}

	In general, we expect that as our data gets closer together, the support of $Q$ gets smaller. However, Figure \ref{fig:phaseplotn3} demonstrates that this is not always the case.

	\subsection{The shape of \texorpdfstring{$C_1$}{C1} for normal densities}
	Here we present some bounds on $C_1$ when using a normal component density.

	\begin{theorem} \label{thm:shapeofC1normala}
		Let $f_\theta(x)$ be a normal density with mean $\theta$ and variance $\sigma^2$ and let $\vect{x} = (x_1,\dots,x_n)$ be the data for which we are finding a maximum likelihood density estimate. Write $\bar{x}$ for the mean of $\vect{x}$ and define $\vect{\bar{x}} = (\bar{x},\dots,\bar{x})$.
		
		Then $\vect{x} \notin C_1$ if
		$$|| \vect{x} - \vect{\bar{x}}|| > \sigma \sqrt{n}.$$
	\end{theorem}
	\begin{proof}
		We consider the function $L_2: \mathbb{R}^2 \mapsto \mathbb{R}$ where
		\begin{equation}\label{smallndata:eq:L2}
			L_2(\theta_1,\theta_2;p,\vect{x},\sigma) = \sum_{i = 1}^n \log\left[ \frac{1}{\sigma \sqrt{2 \pi}} \left( p \euler^{-(x_i - \theta_1)^2/2\sigma^2} + (1-p) \euler^{-(x_i - \theta_2)^2/2\sigma^2}\right) \right],
		\end{equation}
		$0<p<1$, $\sigma> 0$, $\vect{x} \in \mathbb{R}^n$.
		This is the log likelihood \eqref{smallndata:eq:likelihood} where $Q$ is restricted to having only two probability masses at locations $\theta_1$ and $\theta_2$ with probability masses $p$ and $1 - p$ respectively. 
		%Note that we treat the location of the masses as variables but think of the associated weights as fixed. 
	%	We do this since for any choice of $\theta_1 = \theta_2$, $L_2$ is constant along $p$. We intend to use derivatives to find stationary points of $L_2$ and then use second derivative tests to classify these points as either local maximums or not.
		
		If a sample, $\vect{x}$, lies in $C_1$ then for any $p$, $L_2(\theta_1,\theta_2;p,\vect{x},\sigma)$ attains its maximum value at some point where $\theta_1 = \theta_2$. Thus we are primarily interested in the stationary points of $L_2$ that occur along the line $\theta_1 = \theta_2$.
		
		Taking first derivatives of $L_2$ and setting $\theta_1 = \theta_2 = \theta$ we have
		\begin{equation}
		\left. \nabla L_2 \right|_{\theta_1 = \theta_2 = \theta} = \left(\frac{p}{\sigma^2} \sum_{i=1}^n(x_i - \theta),\frac{1-p}{\sigma^2} \sum_{i=1}^n(x_i - \theta)\right).
		\end{equation}
		This is zero only at $\theta = \bar{x}$. At this point, the Hessian matrix of $L_2$ is
		\begin{equation}
		H(\bar{x},\bar{x}) = \frac{1}{\sigma^4} \begin{pmatrix}
		-p n \sigma^2 + p(1-p) \sum_{i=1}^n (x_i - \bar{x})^2 &  -p(1-p) \sum_{i=1}^n (x_i - \bar{x})^2\\
		 -p(1-p) \sum_{i=1}^n (x_i - \bar{x})^2 & -(1-p) n \sigma^2 + p(1-p) \sum_{i=1}^n (x_i - \bar{x})^2
		\end{pmatrix}
		\end{equation}
		The determinant of this matrix is
		\begin{equation}
		|H(\bar{x},\bar{x})| = \frac{np(1-p)}{\sigma^6} \left(n\sigma^2 - \sum_{i=1}^n (x_i - \bar{x})^2 \right)
		\end{equation}
		which is negative for 
		\begin{equation}
		\sum_{i=1}^n (x_i - \bar{x})^2 > n \sigma^2,
		\end{equation}
		or equivalently
		\begin{equation}
		||\vect{x} - \vect{\bar{x}}|| > \sigma \sqrt{n}.
		\end{equation}
		
		Since a negative determinant corresponds to a saddle point, when $||\vect{x} - \vect{\bar{x}}|| > \sigma \sqrt{n}$, $L_2$ has no maximum along $\theta_1 = \theta_2$ and so $\vect{x} \notin C_1$.
	\end{proof}

	EXPAND ON SHAPE CYLINDER ETC

	This bound is shown for $n=3$ in Figure \ref{fig:C1bound}. We note that while it is necessary that $||\vect{x} - \vect{\bar{x}}|| \leq \sigma \sqrt{n}$ for $x$ to be in $C_1$, it is not sufficient. We will now adapt part of a theorem from \cite{Lindsay1983a-he} \cite{Lindsay1983a-he} to give us a sufficient condition for $\vect{x}$ to be in $C_1$.

	\begin{figure}[ht]
		\centering
		\includegraphics[width= \textwidth]{SmallNData/Sigma1n3res128width3_ellipse}
		\caption{text}
		\label{fig:C1bound}
	\end{figure}

	\begin{theorem}[\cite{Lindsay1983a-he}] \label{thm:shapeofC1normalb}
		Let $f_\theta(x)$ be a normal density with mean $\theta$ and variance $\sigma^2$ and let $\vect{x} = (x_1,\dots,x_n)$ be the data for which we are finding a maximum likelihood density estimate. Write $\bar{x}$ for the mean of $\vect{x}$. If the mixture quadratic
	%	$$\expect_\mu [(X - y_1)(X - y_K)]$$
		$$M(\theta) = (\theta - x_{(1)})(\theta - x_{(n)}) + \sigma^2$$
		is strictly positive on $[x_{(1)},x_{(n)}]$ then the mixing distribution with mass one at $\bar{x}$ must maximize the likelihood. 
	\end{theorem}
	\begin{corollary}
		If $x_{(n)} - x_{(1)} < 2\sigma$ then $\vect{x} \in C_1$.
	\end{corollary}

	SHOW BOUND in FIGURE

	\subsubsection{Probabilities}

	We now make the assumption that $\vect{x}$ is made up of i.i.d. random variables, $x_i$, which have distribution
	$$x_i \sim N(\mu,\sigma_1^2)$$
	for $i = 1,\dots,n$. Our component density, $f_\theta$, is normal with variance $\sigma_2^2$. From Theorem \ref{thm:shapeofC1normala},
	\begin{align*}
	p_u &= \prob \left( \sum_{i=1}^n (x_i - \bar{x})^2 \leq n \sigma_2^2  \right)
	\end{align*}
	is an upper bound to $\prob(\vect{x} \in C_1)$. Writing $s^2$ for the unbiased sample variance
	\begin{align*}
	p_u 
	%&= \prob\left( \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \leq \frac{n\sigma}{n-1}\right)\\
	&= \prob\left( s^2 \leq \frac{n \sigma_2^2}{n-1}\right)\\
	&= \prob\left( \frac{(n-1)s^2}{\sigma_1^2} \leq \frac{n \sigma_2^2}{\sigma_1^2}\right)\\
	&= \prob\left(\chi_{n-1}^2  \leq \frac{n \sigma_2^2}{\sigma_1^2} \right)
	\end{align*}
	where $\chi_{n-1}^2$ is chi-squared with $n-1$ degrees of freedom.

	From Theorem \ref{thm:shapeofC1normalb}, an lower bound to $\prob(\vect{x} \in C_1)$ is
	\begin{align*}
	p_l &= \prob(x_{(n)} - x_{(1)} < 2\sigma)\\
		&\geq \prob(\mu - \sigma < x_{(1)} < x_{(n)} < \mu + \sigma )\\
		&=  \left(\int_{\mu - \sigma}^{\mu + \sigma} f(x) \intd x \right)^n\\
		&= \left( \erf\left(\frac{\sigma_2}{\sqrt{2}\sigma_1}\right)\right)^n
	\end{align*}





	%%%%%% Conjecture turned out to be false so have removed this %%%%%%%
	%The proof of Theorem \ref{conj:shapeofC1normalb} will require the following lemma.	
	%\begin{lemma}
	%	A sample, $\vect{x}$, lies in $C_1$ if $L_2(\theta_1,\theta_2;p,\vect{x},\sigma)$ attains its maximum value at some point where $\theta_1 = \theta_2$.
	%\end{lemma}
	%\begin{proof}
	%	This is a Corollary of Lemma \ref{lem:sequencestrictlyincreasing}. If $\vect{x} \in C_k$ for some $k > 1$ then $\alpha_2 > \alpha_1$ and so $L_2(\theta_1,\theta_2;p,\vect{x},\sigma)$ cannot attain its maximum value at some point where $\theta_1 = \theta_2$.
	%\end{proof}

	%We are now ready to prove Theorem \ref{conj:shapeofC1normalb}.
	%\begin{conjecture} \label{conj:shapeofC1normalb}
	%	Let $f_\theta(x)$ be a normal density with mean $\theta$ and variance $\sigma^2$ and let $\vect{x} = (x_1,\dots,x_n)$ be the data for which we are finding a maximum likelihood density estimate. Write $\bar{x}$ for the mean of $\vect{x}$ and define $\vect{\bar{x}} = (\bar{x},\dots,\bar{x})$.
	%	
	%	Then $\vect{x} \in C_1$ if
	%	$$|| \vect{x} - \vect{\bar{x}}|| \leq \sigma \sqrt{n}.$$
	%\end{conjecture}
	%\begin{proof}
	%	We know that $(\bar{x},\bar{x})$ is a local maximum of $L_2(\theta_1,\theta_2;p,\vect{x},\sigma)$ when
	%	$$|| \vect{x} - \vect{\bar{x}}|| < \sigma \sqrt{n}.$$
	%	
	%	Sufficient to show that no other stationary points exist in this case (as local max must exist).
	%	
	%	Sufficient to show that only one local maximum exists
	%	
	%	Idea: some sort of polar coordinates around $(\bar{x},\bar{x})$.
	%	
	%	Use directional derivative to show only one maximum exists (up to symmetry)... no idea how though :P Actually not true for some symmetrical data...I think...
	%	
	%	At local max, what is directional derivative in any direction? If it's negative/0 then at global max...
	%	
	%	Read Lindsay. Lemma should be something like local maximum is global maximum.
	%	Hold on - but definitely not at global max at saddle point so what's happening?
		
	%	We observe that if
	%	\begin{equation}
	%	\sum_{i=1}^n (x_i - \bar{x}) < n \sigma^2,
	%	\end{equation}
	%	then 
	%	\begin{equation}
	%	\frac{\partial^2 L_2}{\partial \theta_1^2} (\bar{x},\bar{x}) = \frac{1}{\sigma^4} \left( -p n \sigma^2 + p(1-p) \sum_{i=1}^n (x_i - \bar{x})^2 \right) < 0.
	%	\end{equation}
	%	So when $||\vect{x} - \vect{\bar{x}}|| < \sigma \sqrt{n}$, the point $(\bar{x},\bar{x})$ is a local maximum of $L$.
	%	
	%	We need to show that it is a global maximum. Lemma?
	%	
	%	It remains to determine the boundary case. Lemma?
	%\end{proof}


	%\subsection{Normal with one component}
	%
	%%obviously add in real formula for normal later
	%Consider the problem where we restrict $Q$ to concentrating all of its mass at one point, $\phi$, and where our component density, $f_\theta(x)$, is normal with mean $\theta$ and variance $\sigma$. Then we can write the log likelihood as
	%\begin{align*}
	%L(\phi;\vect{x}) &= \sum_{i=1}^n \log \left(\frac{1}{2\sigma} e^{-(x_i - \phi)^2/\sigma^2}\right)\\
	%&= -n\log(2\sigma) - \frac{1}{\sigma^2}\sum_{i = 1}^n (x_i - \phi)^2
	%\end{align*}
	%In order to maximize this we need to choose $\phi$ to minimize
	%\begin{equation}
	%\sum_{i=1}^n (x_i - \phi)^2.
	%\label{eq:sumofsquares}
	%\end{equation}
	%If we write $\vect{\Phi}(\phi) = \{\phi,\dots,\phi\}$ then the $\phi$ that minimizes \eqref{eq:sumofsquares} is the $\phi$ that minimizes the Euclidean distance between $\vect{x}$ and $\vect{\Phi}(\phi)$. %We note that $\min_\phi(L(\phi ;\vect{x}))$ is a function of the distance between $\vect{x}$ and the line $x_1 = x_2 = \dots = x_n$.
	%
	%%Speculative
	%
	%We now consider the case where $Q$ has $2$ points of mass, $p_j$ at $\phi_j$ for $j = 1,...,m$. The log likelihood can be written as
	%\begin{align*}
	%L(Q;\vect{x}) &= \sum_{i=1}^n \log \left( \frac{1}{2\sigma} \sum_{j = 1}^2 p_j e^{(-x_i - \phi_j)^2/\sigma^2}\right)\\
	%&=-n\log(2\sigma) + \sum_{i=1}^n \log\left(\sum_{j=1}^2 p_j e^{(-x_i - \phi_j)^2/\sigma^2} \right)\\
	%&=-n\log(2\sigma) + \sum_{i=1}^n \log\left(p_1 e^{-(x_i - \phi_1)^2/\sigma^2} + p_2 e^{(-x_i - \phi_2)^2/\sigma^2} \right)\\
	%%&=-n\log(2\sigma) +\sum_{i = 1}^n \log\left(p_1 e^{-(x_i - \phi_1)^2/\sigma^2}\right) + \sum_{i=1}^n \log \left(1+ \frac{p_2}{p_1}\frac{e^{-(x_i - \phi_2)^2/\sigma^2}}{e^{-(x_i - \phi_1)^2/\sigma^2}} \right)\\
	%%&= -n\log(2\sigma) - \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \phi_1)^2 + n\log(p_1) + \sum_{i=1}^n \log \left(1+ \frac{p_2}{p_1} e^{((x_i - \phi_1)^2 -(x_i - \phi_2)^2)/\sigma^2} \right)
	%\end{align*}

	%	We now consider the case where $Q$ has $m$ points of mass, $p_j$ at $\phi_j$ for $j = 1,...,m$. The likelihood can be written as
	%	\begin{align*}
	%	L(Q;\vect{x}) &= \sum_{i=1}^n \log \left( \frac{1}{2\sigma} \sum_{j = 1}^m p_j e^{(-x_i - \phi_j)^2/\sigma^2}\right)\\
	%		&=-n\log(2\sigma) + \sum_{i=1}^n \log\left(\sum_{j=1}^m p_j e^{(-x_i - \phi_j)^2/\sigma^2} \right)\\
	%		&=-n\log(2\sigma) + \sum_{i=1}^n \log\left(p_1 e^{-(x_i - \phi_1)^2/\sigma^2} + \sum_{j=2}^m p_j e^{(-x_i - \phi_j)^2/\sigma^2} \right)\\
	%		&=-n\log(2\sigma) +\sum_{i = 1}^n \log\left(p_1 e^{-(x_i - \phi_1)^2/\sigma^2}\right) + \sum_{i=1}^n \log \left(1+ \sum_{j=2}^m \frac{ p_j e^{(-x_i - \phi_j)^2/\sigma^2}}{p_1 e^{-(x_i - \phi_1)^2/\sigma^2}} \right)\\
	%		&= -n\log(2\sigma) - \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \phi_1)^2 + n\log(p_1) + \sum_{i=1}^n \log \left(1+ \sum_{j=2}^m \frac{ p_j e^{(-x_i - \phi_j)^2/\sigma^2}}{p_1 e^{-(x_i - \phi_1)^2/\sigma^2}} \right)
	%	\end{align*}

	%For $\sigma = 1$ this has partial derivatives
	%\begin{align*}
	%\frac{\partial L}{\partial \phi_1} &= -2 p_1 \sum_{i=1}^n  \frac{(\phi_1 - x_i) e^{-(\phi_1 - x_i)^2}}{p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{(-x_i - \phi_2)^2}}\\
	%\frac{\partial L}{\partial \phi_2} &= -2 p_2 \sum_{i=1}^n  \frac{(\phi_2 - x_i) e^{-(\phi_2 - x_i)^2}}{p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{(-x_i - \phi_2)^2}}\\
	%\frac{\partial L}{\partial p_1} &=  \sum_{i=1}^n  \frac{e^{-(\phi_1 - x_i)^2} - e^{-(\phi_2 - x_i)^2}}{p_1 e^{-(x_i - \phi_1)^2} + (1-p_1) e^{(-x_i - \phi_2)^2}}\\
	%\frac{\partial^2 L}{\partial \phi_1^2} &= -2p_1\sum_{i=1}^n \frac{2p_1 (\phi_1-x_i)^2 e^{-2(\phi_1 - x_i)^2}}{(p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{-(x_i - \phi_2)^2})^2} + 
	%\frac{e^{-(\phi_1 - x_i)^2}(1 -2 (\phi_1 - x_i)^2)}{p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{-(x_i - \phi_2)^2}}\\
	%\frac{\partial^2 L}{\partial \phi_2^2} &= -2p_2\sum_{i=1}^n \frac{2p_2 (\phi_2-x_i)^2 e^{-2(\phi_2 - x_i)^2}}{(p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{-(x_i - \phi_2)^2})^2} + 
	%\frac{e^{-(\phi_2 - x_i)^2}(1 -2 (\phi_2 - x_i)^2)}{p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{-(x_i - \phi_2)^2}}\\
	%\frac{\partial^2 L}{\partial p_1^2} &=  -\sum_{i=1}^n  \frac{\left( e^{-(\phi_1 - x_i)^2} - e^{-(\phi_2 - x_i)^2}\right)^2}{\left(p_1 e^{-(x_i - \phi_1)^2} + (1-p_1) e^{(-x_i - \phi_2)^2}\right)^2}\\
	%\frac{\partial^2 L}{\partial \phi_1 \partial \phi_2} &= -4 p_1 p_2 \sum_{i=1}^n  \frac{(\phi_1 - x_i)(\phi_2 - x_i) e^{-(\phi_1 - x_i)^2-(\phi_2 - x_i)^2}}{(p_1 e^{-(x_i - \phi_1)^2} + p_2 e^{-(x_i - \phi_2)^2})^2}\\
	%\frac{\partial^2 L}{\partial p_1 \partial \phi_1} &= -2 \sum_{i=1}^n \frac{(\phi_1 - x)e^{}}{}\\
	%\frac{\partial^2 L}{\partial p_1 \partial \phi_2} &= -2 \sum_{i=1}^n
	%\end{align*}
	%
	%at $\phi_1 = \phi_2 = \phi$ these are
	%
	%\begin{align*}
	%\left. \frac{\partial L}{\partial \phi_1} \right\rvert_{\phi_1,\phi_2 = \phi} &= -2 p_1 \sum_{i=1}^n  (\phi - x_i)\\
	%\left.\frac{\partial L}{\partial \phi_2}\right\rvert_{\phi_1, \phi_2 = \phi}  &= -2 p_2 \sum_{i=1}^n  (\phi - x_i)\\
	%\left.\frac{\partial^2 L}{\partial \phi^2}\right\rvert_{\phi_1, \phi_2 = \phi}  &= n + 4p_1p_2\sum_{i=1}^n (\phi-x_i)^2\\
	%\left.\frac{\partial^2 L}{\partial \phi_1 \partial \phi_2}\right\rvert_{\phi_1,\phi_2 = \phi}  &= -4 p_1 p_2 \sum_{i=1}^n (\phi - x_i)^2\\
	%\left.\frac{\partial^2 L}{\partial \phi_2^2}\right\rvert_{\phi_1, \phi_2 = \phi}  &= n + 4p_1p_2\sum_{i=1}^n  (\phi-x_i)^2
	%\end{align*}
	%
	%\begin{equation}
	%D = \left(n+4p_1p_2\sum_{i=1}^n (\phi - x_i)^2\right)^2 - \left(4p_1p_2\sum_{i=1}^n (\phi - x_i)^2\right)^2>0
	%\end{equation}
	%and
	%\begin{equation}
	%\frac{\partial^2 L}{\partial \phi^2} > 0
	%\end{equation}
	%
	%so $\phi_1 = \phi_2 = \phi$ is local minimum.
	%
	%At $\phi_1 = \phi_2 = \phi$ the Hessian Matrix is
	%\begin{equation}
	%H = 
	%\begin{pmatrix}
	%n+4p_1p_2\sum_{i = 1}^n (\phi-x_i)^2 & -4 p_1 p_2 \sum_{i=1}^n (\phi - x_i)^2 & -2\sum_{i=1}^n (\phi - x_i)\\
	%-4 p_1 p_2 \sum_{i=1}^n (\phi - x_i)^2& n + 4p_1p_2\sum_{i=1}^n  (\phi-x_i)^2 & 2\sum_{i=1}^n (\phi - x_i)\\
	%-2\sum_{i=1}^n (\phi - x_i)& 2\sum_{i=1}^n (\phi - x_i)& 0
	%\end{pmatrix}
	%\end{equation}
	%
	%This is a Hermitian matrix and so we can use Sylvester's criterion to determine when it is positive definite.
	%
	%Clearly
	%\begin{equation}
	%n+4p_1p_2\sum_{i = 1}^n (\phi-x_i)^2
	%\end{equation}
	%is positive and
	%\begin{equation}
	%\begin{pmatrix}
	%n+4p_1p_2\sum_{i = 1}^n (\phi-x_i)^2 & -4 p_1 p_2 \sum_{i=1}^n (\phi - x_i)^2\\
	%-4 p_1 p_2 \sum_{i=1}^n (\phi - x_i)^2& n + 4p_1p_2\sum_{i=1}^n  (\phi-x_i)^2
	%\end{pmatrix}
	%\end{equation}
	%has positive determinant.
	%
	%However,
	%\begin{equation}
	%\det(H) = -8n \left(\sum_{i=1}^n (x_i - \phi) \right)^2
	%\end{equation}
	%which is negative. So the second derivative test is inconclusive.

	%%%%%Extra stuff that doesn't quite fit in yet%%%%%